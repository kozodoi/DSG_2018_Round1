{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import spearmanr\n",
    "import lightgbm as lgb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data name (used for saving files too)\n",
    "data_name = \"data_v4_0_60_under\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "data = pd.read_csv(\"../data/prepared/\" + str(data_name) + \".csv\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data name (used for saving files too)\n",
    "data_name = \"data_v4_0_60_under_wlp_lm_bm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop high recency from train\n",
    "data = data[(data.Week == 121) | (data.Recency1 < data.Recency1.max())]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PREPARATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD MORE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BuySell dummy\n",
    "data[\"Buy\"] = 0\n",
    "data[\"Buy\"][data.BuySell == \"Buy\"] = 1\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last week sum (CUST)\n",
    "tmp = data.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"Week\"], as_index = True)\n",
    "tmp = tmp.CustomerInterest.sum().reset_index()\n",
    "tmp.columns = [\"CustomerIdx\", \"Week\", \"LastWeekCustSum\"]\n",
    "tmp[\"Week\"] = tmp.Week + 1\n",
    "data = data.merge(tmp, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "print(data.shape)\n",
    "\n",
    "# add last week sum (BOND)\n",
    "tmp = data.sort_values(by = [\"Week\"], ascending = True).groupby([\"IsinIdx\", \"Week\"], as_index = True)\n",
    "tmp = tmp.CustomerInterest.sum().reset_index()\n",
    "tmp.columns = [\"IsinIdx\", \"Week\", \"LastWeekBondSum\"]\n",
    "tmp[\"Week\"] = tmp.Week + 1\n",
    "data = data.merge(tmp, how = \"left\", on = [\"IsinIdx\", \"Week\"])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD PRICE AND NOTIONAL\n",
    "\n",
    "tmp = pd.read_csv(\"../data/prepared/price_notional.csv\", compression = \"gzip\")\n",
    "\n",
    "# add last week price sum (CUST)\n",
    "tmp2 = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"Week\"], as_index = True)\n",
    "tmp2 = tmp2.Price.sum().reset_index()\n",
    "tmp2.columns = [\"CustomerIdx\", \"Week\", \"LastWeekCustPriceSum\"]\n",
    "tmp2[\"Week\"] = tmp2.Week + 1\n",
    "data = data.merge(tmp2, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "print(data.shape)\n",
    "\n",
    "# add last week notional sum (CUST)\n",
    "tmp2 = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"Week\"], as_index = True)\n",
    "tmp2 = tmp2.NotionalEUR.sum().reset_index()\n",
    "tmp2.columns = [\"CustomerIdx\", \"Week\", \"LastWeekCustNotionalSum\"]\n",
    "tmp2[\"Week\"] = tmp2.Week + 1\n",
    "data = data.merge(tmp2, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD LAST MONTH SUMS\n",
    "\n",
    "# merge month number\n",
    "tmp = pd.read_csv(\"../data/raw/Trade.csv\")[[\"TradeDateKey\"]]\n",
    "tmp = tmp.drop_duplicates()\n",
    "tmp[\"TradeDateKey\"] = pd.to_datetime(tmp[\"TradeDateKey\"], format = '%Y%m%d')\n",
    "tmp[\"Week\"] = (tmp.TradeDateKey.dt.year - 2016) * 52 + (tmp.TradeDateKey.dt.week)\n",
    "tmp[\"CumMonth\"] = (tmp.TradeDateKey.dt.year - 2016) * 12 + (tmp.TradeDateKey.dt.month)\n",
    "tmp = tmp[[\"Week\", \"CumMonth\"]]\n",
    "tmp = tmp.drop_duplicates()\n",
    "tmp = tmp.groupby(\"Week\").CumMonth.min().reset_index()\n",
    "data = data.merge(tmp, how = \"left\", on = \"Week\")\n",
    "data[\"CumMonth\"][data.Week == 121] = 28\n",
    "\n",
    "# add last month mean (CUST)\n",
    "tmp = data.sort_values(by = [\"CumMonth\"], ascending = True).groupby([\"CustomerIdx\", \"CumMonth\"], as_index = True)\n",
    "tmp = tmp.CustomerInterest.sum().reset_index()\n",
    "tmp.columns = [\"CustomerIdx\", \"CumMonth\", \"LastMonthCustSum\"]\n",
    "tmp[\"CumMonth\"] = tmp.CumMonth + 1\n",
    "data = data.merge(tmp, how = \"left\", on = [\"CustomerIdx\", \"CumMonth\"])\n",
    "print(data.shape)\n",
    "\n",
    "# add last month mean (BOND)\n",
    "tmp = data.sort_values(by = [\"CumMonth\"], ascending = True).groupby([\"IsinIdx\", \"CumMonth\"], as_index = True)\n",
    "tmp = tmp.CustomerInterest.sum().reset_index()\n",
    "tmp.columns = [\"IsinIdx\", \"CumMonth\", \"LastMonthBondSum\"]\n",
    "tmp[\"CumMonth\"] = tmp.CumMonth + 1\n",
    "data = data.merge(tmp, how = \"left\", on = [\"IsinIdx\", \"CumMonth\"])\n",
    "print(data.shape)\n",
    "\n",
    "# drop month\n",
    "del data[\"CumMonth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD BOND MATURITY DATA\n",
    "\n",
    "# import\n",
    "bond = pd.read_csv(\"../data/prepared/data_bond_v1.csv\", compression = \"gzip\")\n",
    "bond = bond[['IsinIdx', 'MaturityWeek', 'IssueWeek']]\n",
    "\n",
    "# merge\n",
    "data = data.merge(bond, on = \"IsinIdx\", how = \"left\")\n",
    "\n",
    "# compute week differences\n",
    "data[\"MaturityWeek\"]    = data.MaturityWeek - data.Week\n",
    "data[\"IssueWeek\"]       = data.Week - data.IssueWeek\n",
    "data[\"MaturityPercent\"] = (data.Week - data.IssueWeek) / (data.MaturityWeek - data.IssueWeek)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKS AND PREPARATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "print(\"Dimensions:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA\n",
    "nas = data.isnull().sum()\n",
    "nas[nas > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features\n",
    "excluded_features = [\"PredictionIdx\", \"CustomerIdx\", \"IsinIdx\", \"BuySell\", \"CustomerInterest\",\n",
    "                     \"Frequecny1isLowerFrequency2\", \"Frequecny2isLowerFrequency4\",\n",
    "                     \"IsinIdx_mode\", \"TickerIdx_mode\", \"IndustrySector_mode\", \n",
    "                     \"IndustrySubgroup_mode\", \"MarketIssue_mode\",\n",
    "                     \"Corporation\", \"Activity_mode\"]\n",
    "features = [var for var in data.columns if var not in excluded_features]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PARTITIONING\n",
    "\n",
    "# training\n",
    "X_train = data.loc[data.Week < 120]\n",
    "y_train = data.loc[data.Week < 120].CustomerInterest\n",
    "\n",
    "# validation\n",
    "X_valid = data.loc[data.Week == 120]\n",
    "y_valid = data.loc[data.Week == 120].CustomerInterest\n",
    "\n",
    "# test set\n",
    "test = data.loc[data.Week == 121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA in test\n",
    "nas = test.isnull().sum()\n",
    "nas[nas > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MODELING - STAGE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "\n",
    "# parallel settings\n",
    "cores = 16\n",
    "\n",
    "# learner settings\n",
    "metric   = \"auc\"\n",
    "verbose  = 250\n",
    "stopping = 150\n",
    "\n",
    "# lightGBM\n",
    "gbm = lgb.LGBMClassifier(n_estimators     = 10000,\n",
    "                         learning_rate    = 0.005,\n",
    "                         num_leaves       = 70,\n",
    "                         colsample_bytree = 0.8,\n",
    "                         subsample        = 0.9,\n",
    "                         max_depth        = 7,\n",
    "                         reg_alpha        = 0.1,\n",
    "                         reg_lambda       = 0.1,\n",
    "                         min_split_gain   = 0.01,\n",
    "                         min_child_weight = 2,\n",
    "                         random_state     = 42,\n",
    "                         num_threads      = cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lightGBM\n",
    "gbm = gbm.fit(X_train[features], y_train, \n",
    "              eval_set = [(X_train[features], y_train), \n",
    "                          (X_valid[features], y_valid)], \n",
    "              eval_metric = metric, verbose = verbose, \n",
    "              early_stopping_rounds = stopping)\n",
    "    \n",
    "# save number of iterations\n",
    "num_iters = gbm.best_iteration_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### VARIABLE IMPORTANCE\n",
    "\n",
    "# load variable importance\n",
    "importance = pd.DataFrame()\n",
    "importance[\"feature\"] = features\n",
    "importance[\"importance\"] = gbm.feature_importances_\n",
    "\n",
    "# plot variable importance\n",
    "plt.figure(figsize = (10, 12))\n",
    "sns.barplot(x = \"importance\", y = \"feature\", data = importance.sort_values(by = \"importance\", ascending = False))\n",
    "plt.title('LGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot as pdf\n",
    "#plt.savefig(\"../var_importance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PREDICTION\n",
    "\n",
    "### VALID DATA\n",
    "\n",
    "# predict validation set\n",
    "pred_valid = X_valid[[\"CustomerIdx\", \"IsinIdx\", \"BuySell\", \"Week\", \"CustomerInterest\"]]\n",
    "pred_valid[\"TARGET\"] = gbm.predict_proba(X_valid[features], num_iteration = num_iters)[:, 1]\n",
    "auc = roc_auc_score(y_valid, pred_valid.TARGET)\n",
    "\n",
    "# check rank correlation with the best submission\n",
    "best = pd.read_csv(\"../pred_valid/auc851538_data_v4_0_60_under_wlp_lm_bm_mff_lgb.csv\")\n",
    "best.columns = ['CustomerIdx', 'IsinIdx', 'BuySell', 'Week', 'CustomerInterest', 'TARGET_best']\n",
    "best = best.merge(pred_valid[[\"CustomerIdx\", \"IsinIdx\", \"BuySell\", \"TARGET\"]], \n",
    "                  how = \"right\", on = [\"CustomerIdx\", \"IsinIdx\", \"BuySell\"])\n",
    "print(spearmanr(best.TARGET, best.TARGET_best))\n",
    "\n",
    "# export CSV\n",
    "pred_valid.to_csv(\"../pred_valid/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb.csv\", \n",
    "                  index = False, float_format = \"%.8f\")\n",
    "\n",
    "\n",
    "### TEST DATA\n",
    "\n",
    "# predict test set\n",
    "test[\"TARGET\"] = gbm.predict_proba(test[features], num_iteration = num_iters)[:, 1]\n",
    "\n",
    "# check rank correlation with the best submission\n",
    "best = pd.read_csv(\"../submissions/auc851538_data_v4_0_60_under_wlp_lm_bm_mff_lgb_2stage.csv\")\n",
    "best = best.merge(test[[\"PredictionIdx\", \"TARGET\"]], how = \"left\", on = \"PredictionIdx\")\n",
    "print(spearmanr(best[\"TARGET\"], best.CustomerInterest))\n",
    "\n",
    "# export CSV\n",
    "subm = best[[\"PredictionIdx\", \"TARGET\"]]\n",
    "subm.columns = [\"PredictionIdx\", \"CustomerInterest\"]\n",
    "subm.to_csv(\"../submissions/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb_1stage.csv\", \n",
    "            index = False, float_format = \"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MODELING - STAGE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep top features\n",
    "#top = 100\n",
    "#features = list(importance[\"feature\"][0:np.min([top, len(features)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full data as train\n",
    "X_train = data.loc[data.Week <= 120]\n",
    "y_train = data.loc[data.Week <= 120].CustomerInterest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrain model with the best iters\n",
    "\n",
    "# lightGBM\n",
    "gbm = lgb.LGBMClassifier(n_estimators     = num_iters,\n",
    "                         learning_rate    = 0.005,\n",
    "                         num_leaves       = 70,\n",
    "                         colsample_bytree = 0.8,\n",
    "                         subsample        = 0.9,\n",
    "                         max_depth        = 7,\n",
    "                         reg_alpha        = 0.1,\n",
    "                         reg_lambda       = 0.1,\n",
    "                         min_split_gain   = 0.01,\n",
    "                         min_child_weight = 2,\n",
    "                         random_state     = 42,\n",
    "                         num_threads      = cores)\n",
    "\n",
    "# train lightGBM\n",
    "gbm = gbm.fit(X_train[features], y_train, \n",
    "              eval_set = [(X_train[features], y_train)], \n",
    "              eval_metric = metric, verbose = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set\n",
    "test[\"TARGET\"] = gbm.predict_proba(test[features], num_iteration = num_iters)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rank correlation with the best submission\n",
    "best = pd.read_csv(\"../submissions/auc851538_data_v4_0_60_under_wlp_lm_bm_lgb_2stage.csv\")\n",
    "best = best.merge(test[[\"PredictionIdx\", \"TARGET\"]], how = \"left\", on = \"PredictionIdx\")\n",
    "spearmanr(best[\"TARGET\"], best.CustomerInterest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV\n",
    "subm = best[[\"PredictionIdx\", \"TARGET\"]]\n",
    "subm.columns = [\"PredictionIdx\", \"CustomerInterest\"]\n",
    "subm.to_csv(\"../submissions/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb_2stage.csv\", \n",
    "            index = False, float_format = \"%.8f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
