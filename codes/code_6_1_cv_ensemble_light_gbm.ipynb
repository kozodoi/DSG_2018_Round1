{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import scipy.stats\n",
    "import os\n",
    "from functions.smooth_stat import smooth_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA PARTITIONING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "#train = pd.read_csv(\"../data/prepared/train_no_holding.csv\")\n",
    "#test  = pd.read_csv(\"../data/prepared/test_no_holding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values('Week', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS-VALIDATION TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "\n",
    "# learner settings\n",
    "metric   = \"auc\"\n",
    "verbose  = 10\n",
    "stopping = 30\n",
    "seed = 42\n",
    "features = [\"ratio1\", \"ratio2\", \"ratio3\", \"ratio4\", \"ratio5\", \"ratio6\", \"RatioMean\"]\n",
    "n_folds = 3\n",
    "\n",
    "# lgb settings\n",
    "gbm = lgb.LGBMClassifier(n_estimators     = 1000,\n",
    "                         learning_rate    = 0.005,\n",
    "                         num_leaves       = 70,\n",
    "                         colsample_bytree = 0.8,\n",
    "                         subsample        = 0.9,\n",
    "                         max_depth        = 7,\n",
    "                         reg_alpha        = 0.1,\n",
    "                         reg_lambda       = 0.1,\n",
    "                         min_split_gain   = 0.01,\n",
    "                         min_child_weight = 2,\n",
    "                         random_state     = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model(model, train_x, train_y, val_x, val_y):\n",
    "    #create features inside the CV\n",
    "    \n",
    "    # train lightGBM\n",
    "    global verbose\n",
    "    global stopping\n",
    "    global metric\n",
    "    model = model.fit(train_x, train_y, \n",
    "              eval_set = [(train_x, train_y), (val_x, val_y)], \n",
    "              eval_metric = metric, \n",
    "              verbose = verbose, \n",
    "              early_stopping_rounds = stopping)\n",
    "    \n",
    "    # save number of iterations\n",
    "    num_iters = gbm.best_iteration_\n",
    "    best_auc = gbm.best_score_\n",
    "    return (model, num_iters, best_auc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(dataset, fold_count):\n",
    "    fold_size = len(dataset) // fold_count\n",
    "    for fold_id in range(0, fold_count - 2):\n",
    "                fold_start = fold_size * fold_id\n",
    "                fold_end = fold_start + fold_size\n",
    "                print(f'stat {fold_start}  {fold_end}')\n",
    "                train_start = fold_end\n",
    "                train_end = train_start + fold_size\n",
    "                print(f'train {train_start}  {train_end}')\n",
    "                val_start = train_end\n",
    "                val_end = val_start + fold_size\n",
    "            \n",
    "                if fold_id == fold_count - 2:\n",
    "                    val_end = len(dataset)\n",
    "                print(f'val {val_start}  {val_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat 0  22258776\n",
      "train 22258776  44517552\n",
      "val 44517552  66776328\n",
      "stat 22258776  44517552\n",
      "train 44517552  66776328\n",
      "val 66776328  89035104\n",
      "stat 44517552  66776328\n",
      "train 66776328  89035104\n",
      "val 89035104  111293880\n"
     ]
    }
   ],
   "source": [
    "check(train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold_count):\n",
    "    ''' Please, use Xs and y_s sorted by date,\n",
    "    otherwise it can overfitting by looking in the future'''\n",
    "    import pandas as pd\n",
    "    fold_size = 111293880 // fold_count\n",
    "    models = {}\n",
    "    epochs = {}\n",
    "    aucs = {}\n",
    "    feat = {}\n",
    "    for fold_id in range(0, fold_count - 2):\n",
    "            fold_start = fold_size * fold_id\n",
    "            fold_end = fold_start + fold_size\n",
    "            train_start = fold_end\n",
    "            train_end = train_start + fold_size\n",
    "            val_start = train_end\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            if fold_id == fold_count - 2:\n",
    "                val_end = 111293880\n",
    "            dataset = pd.read_csv(\"../data/prepared/train_no_holding.csv\")   \n",
    "            dataset.sort_values('Week', inplace=True)\n",
    "            stats_data = dataset.iloc[fold_start:fold_end,:]\n",
    "            train_data = dataset.iloc[train_start:train_end,:]\n",
    "            val_data = dataset.iloc[val_start:val_end,:]\n",
    "            print(train_data['Week'].max())\n",
    "            del dataset\n",
    "            \n",
    "            # FEATURE CREATION GOES HERE\n",
    "            # Example (in general can be imporved)\n",
    "\n",
    "            # compute historical target ratio\n",
    "            from functions.smooth_stat import smooth_stat\n",
    "            \n",
    "            print('Feature creation')\n",
    "            \n",
    "            # \"CustomerIdx\"\n",
    "            target_feature='CustomerInterest'\n",
    "            # compute target ratio (last 30 weeks)\n",
    "            cust_int0 = stats_data[stats_data[\"Week\"] >= (stats_data[\"Week\"].max()-30)]\n",
    "            cust_int0 = cust_int0[[\"CustomerIdx\", \"CustomerInterest\", \"IsinIdx\"]]\n",
    "            cust_int0 = cust_int0.groupby([\"CustomerIdx\", \"IsinIdx\"], as_index = False).mean()\n",
    "            cust_int0.columns = [\"CustomerIdx\", \"IsinIdx\", \"ratio0\"]\n",
    "                        \n",
    "            # compute target ratio (last 16 weeks)\n",
    "            cust_int1 = stats_data[stats_data[\"Week\"] >= (stats_data[\"Week\"].max()-16)]\n",
    "            cust_int1 = cust_int1[[\"CustomerIdx\", \"CustomerInterest\", \"IsinIdx\"]]\n",
    "            cust_int1 = cust_int1.groupby([\"CustomerIdx\", \"IsinIdx\"], as_index = False).mean()\n",
    "            cust_int1.columns = [\"CustomerIdx\", \"IsinIdx\", \"ratio1\"]\n",
    "            \n",
    "            # compute target ratio (last 8 weeks)\n",
    "            cust_int2 = stats_data[stats_data[\"Week\"] >= (stats_data[\"Week\"].max()-8)]\n",
    "            cust_int2 = cust_int2[[\"CustomerIdx\", \"CustomerInterest\", \"IsinIdx\"]]\n",
    "            cust_int2 = cust_int2.groupby([\"CustomerIdx\", \"IsinIdx\"], as_index = False).mean()\n",
    "            cust_int2.columns = [\"CustomerIdx\", \"IsinIdx\", \"ratio2\"]\n",
    "            \n",
    "            # compute target ratio (last 4 weeks)\n",
    "            cust_int3 = stats_data[stats_data[\"Week\"] >= (stats_data[\"Week\"].max()-4)]\n",
    "            cust_int3 = cust_int3[[\"CustomerIdx\", \"CustomerInterest\", \"IsinIdx\"]]\n",
    "            cust_int3 = cust_int3.groupby([\"CustomerIdx\", \"IsinIdx\"], as_index = False).mean()\n",
    "            cust_int3.columns = [\"CustomerIdx\", \"IsinIdx\", \"ratio3\"]\n",
    "            \n",
    "            # compute target ratio (last 1 week1)\n",
    "            cust_int4 = stats_data[stats_data[\"Week\"] >= (stats_data[\"Week\"].max())]\n",
    "            cust_int4 = cust_int4[[\"CustomerIdx\", \"CustomerInterest\", \"IsinIdx\"]]\n",
    "            cust_int4 = cust_int4.groupby([\"CustomerIdx\", \"IsinIdx\"], as_index = False).mean()\n",
    "            cust_int4.columns = [\"CustomerIdx\", \"IsinIdx\", \"ratio4\"]\n",
    "            \n",
    "            # compute customer target ratio (last 30 weeks)\n",
    "            cust_int5 = stats_data[stats_data[\"Week\"] >= (110-30)]\n",
    "            cust_int5 = cust_int5[[\"CustomerIdx\", \"CustomerInterest\"]]\n",
    "            cust_int5 = cust_int5.groupby([\"CustomerIdx\"], as_index = False).mean()\n",
    "            cust_int5.columns = [\"CustomerIdx\", \"ratio5\"]\n",
    "            \n",
    "            # compute bond target ratio (last 30 weeks)\n",
    "            cust_int6 = stats_data[stats_data[\"Week\"] >= (110-30)]\n",
    "            cust_int6 = cust_int6[[\"IsinIdx\", \"CustomerInterest\"]]\n",
    "            cust_int6 = cust_int6.groupby([\"IsinIdx\"], as_index = False).mean()\n",
    "            cust_int6.columns = [\"IsinIdx\", \"ratio6\"]\n",
    "            \n",
    "            # merge and average all ratios\n",
    "            cust_int = cust_int0.merge(cust_int1, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\"])\n",
    "            cust_int = cust_int.merge(cust_int2,  how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\"])\n",
    "            cust_int = cust_int.merge(cust_int3,  how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\"])\n",
    "            cust_int = cust_int.merge(cust_int4,  how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\"])\n",
    "            cust_int = cust_int.merge(cust_int5,  how = \"left\", on = [\"CustomerIdx\"])\n",
    "            cust_int = cust_int.merge(cust_int6,  how = \"left\", on = [\"IsinIdx\"])\n",
    "            cust_int[\"RatioMean\"] = (cust_int[\"ratio0\"] + cust_int[\"ratio1\"] + cust_int[\"ratio2\"] + \n",
    "                                     cust_int[\"ratio3\"] + cust_int[\"ratio4\"] + cust_int[\"ratio5\"] +\n",
    "                                     cust_int[\"ratio6\"]) / 7\n",
    "            \n",
    "            del stats_data\n",
    "            # merge features\n",
    "            train_data = train_data.merge(cust_int, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\"])\n",
    "            val_data = val_data.merge(cust_int, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\"])\n",
    "            \n",
    "            train_data.fillna(0, inplace=True)\n",
    "            val_data.fillna(0, inplace=True)\n",
    "            \n",
    "            train_x = train_data.drop('CustomerInterest', axis=1)\n",
    "            train_y = train_data['CustomerInterest']\n",
    "            val_x = val_data.drop('CustomerInterest', axis=1)\n",
    "            val_y = val_data['CustomerInterest']\n",
    "            train_y = train_y.astype('int')\n",
    "            val_y = val_y.astype('int')\n",
    "        \n",
    "            ##### END OF EXAMPLE\n",
    "            global features\n",
    "            # TRAINING\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            classifier = LogisticRegression(C=3)\n",
    "            print('Training')\n",
    "            classifier.fit(train_x[features], train_y)\n",
    "            del train_data\n",
    "            print('Validation')\n",
    "            probs = classifier.predict_proba(val_x[features])[:,1]\n",
    "            auc = roc_auc_score(val_y, probs)\n",
    "            \n",
    "        \n",
    "            print(f'_______________________ \\n {fold_id} {auc} \\n ____________________')\n",
    "            \n",
    "            print('Saving_features')\n",
    "            cust_int.to_csv(f'../submissions/cv_validations/cust_int_{fold_id}.csv', index=False)\n",
    "            \n",
    "            print('Saving model')\n",
    "            filename = f'../submissions/cv_validations/log_reg_{fold_id}.sav'\n",
    "            pickle.dump(classifier, open(filename, 'wb'))\n",
    "            \n",
    "                   \n",
    "   # return (models, epochs, aucs, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_mean(aucs):\n",
    "    train_auc = []\n",
    "    valid0_auc = []\n",
    "    test_auc = []\n",
    "    for fold in aucs:\n",
    "        train_auc.append(aucs[fold]['training']['auc'])  \n",
    "        test_auc.append(aucs[fold]['valid_1']['auc'])   \n",
    "    mean_train = np.asarray(train_auc).mean()\n",
    "    mean_test = np.asarray(test_auc).mean()\n",
    "    return({'train_auc':mean_train, 'cv_auc':mean_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.0\n",
      "Feature creation\n",
      "Training\n",
      "Validation\n",
      "_______________________ \n",
      " 0 0.5094741893368767 \n",
      " ____________________\n",
      "Saving_features\n",
      "Saving model\n",
      "72.0\n",
      "Feature creation\n",
      "Training\n",
      "Validation\n",
      "_______________________ \n",
      " 1 0.5095178333947881 \n",
      " ____________________\n",
      "Saving_features\n",
      "Saving model\n",
      "96.0\n",
      "Feature creation\n",
      "Training\n",
      "Validation\n",
      "_______________________ \n",
      " 2 0.5033542099421813 \n",
      " ____________________\n",
      "Saving_features\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "train_fold(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "for fold_id in range(n_folds - 2):\n",
    "    test_t  = pd.read_csv(\"../data/prepared/test_no_holding.csv\")\n",
    "    cust_int = pd.read_csv(f'../submissions/cv_validations/cust_int_{fold_id}.csv')\n",
    "    test_t = test_t.merge(cust_int, how = \"left\", on = \"CustomerIdx\")\n",
    "    test_t.fillna(0, inplace=True)\n",
    "    print('Predicting')\n",
    "    with open(f'../submissions/cv_validations/log_reg_{fold_id}.sav', 'rb') as pickle_file:\n",
    "        classifier = pickle.load(pickle_file)\n",
    "    test_t[\"CustomerInterest\"] = classifier.predict_proba(test_t[features])[:, 1]\n",
    "    test_t[test_t[features].isnull()][\"CustomerInterest\"] = 0\n",
    "    subm = test_t[[\"PredictionIdx\", \"CustomerInterest\"]]\n",
    "    subm.to_csv(f\"{validation_path}/submission_logit_cv{fold_id}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_auc': 0.86230693406824099, 'train_auc': 0.91670941121186211}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "validation_path = \"../submissions/cv_validations/\"\n",
    "\n",
    "if not os.path.exists(validation_path):\n",
    "    os.makedirs(validation_path)\n",
    "\n",
    "# predict\n",
    "for i, model in enumerate(models):   \n",
    "    test_t = test.merge(features_cv[i], how = \"left\", on = \"CustomerIdx\")\n",
    "    test_t[\"CustomerInterest\"] = models[model].predict_proba(test_t[features], num_iteration = epochs[model])[:, 1]\n",
    "    # smart impute \n",
    "    test_t[test_t[features].isnull()][\"CustomerInterest\"] = 0\n",
    "    # export CSV\n",
    "    subm = test_t[[\"PredictionIdx\", \"CustomerInterest\"]]\n",
    "    subm.to_csv(f\"{validation_path}/submission{i}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = pd.read_csv(f\"{validation_path}/submission0.csv\")\n",
    "for sub in range(n_folds):\n",
    "    if(sub==0):\n",
    "        continue\n",
    "    else:\n",
    "        final_sub['CustomerInterest'] += pd.read_csv(f\"{validation_path}/submission{sub}.csv\")[\"CustomerInterest\"]\n",
    "final_sub['CustomerInterest'] /= n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub.to_csv(\"../submissions/cv_light_gbm_edited_the_cv.csv\", index=False, float_format = \"%.8f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
