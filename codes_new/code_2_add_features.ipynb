{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "data = pd.read_csv(\"../data/prepared/data_basic_0.csv\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "print(\"Dimensions:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FUNCTIONS TO CREATE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "def week_idx(date, end_date):\n",
    "    return round((end_date - date).dt.days / 7 + 0.4).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECENCY (TIME SINCE LAST TRADE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FUNCTION TO COMPUTE 6 RECENCY-BASED FEATURES \n",
    "# 1) Time since last customer trade with that bond with the same BuySell direction\n",
    "# 2) Time since last customer trade with that bond with any direction\n",
    "# 3) Time since last customer trade with any bond with the same BuySell direction\n",
    "# 4) Time since last customer trade with any bond with any direction\n",
    "# 5) Time since last trade with that bond by any of the customers with the same BuySell direction\n",
    "# 6) Time since last trade with that bond by any of the customers\n",
    "\n",
    "def compute_recency(data):\n",
    "\n",
    "    tmp = data[data.CustomerInterest == 1]\n",
    "    \n",
    "    tmp[\"PrevWeek\"] = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).Week.shift(1)\n",
    "    tmp[\"CurrWeek\"] = tmp.Week\n",
    "\n",
    "    data = data.merge(tmp, how = \"left\")\n",
    "\n",
    "    data[\"CurrWeek\"] = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).CurrWeek.fillna(method = \"ffill\")\n",
    "    data[\"PrevWeek\"] = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).PrevWeek.fillna(method = \"bfill\")\n",
    "    data[\"PrevWeek\"][data.PrevWeek.isnull()] = data[\"CurrWeek\"]\n",
    "    data[\"PrevWeek\"][data.PrevWeek >= data.Week] = None\n",
    "    \n",
    "    data[\"Recency1\"] = data[\"Week\"] - data[\"PrevWeek\"]\n",
    "    data[\"Recency1\"][data.Recency1 <= 0] = None\n",
    "    del data[\"PrevWeek\"], data[\"CurrWeek\"]\n",
    "\n",
    "    Recency2 = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency2.columns = [\"CustomerIdx\", \"IsinIdx\", \"Week\", \"Recency2\"]\n",
    "    data = data.merge(Recency2, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    Recency3 = data.groupby([\"CustomerIdx\", \"BuySell\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency3.columns = [\"CustomerIdx\", \"BuySell\", \"Week\", \"Recency3\"]\n",
    "    data = data.merge(Recency3, how = \"left\", on = [\"CustomerIdx\", \"BuySell\", \"Week\"])\n",
    "    \n",
    "    Recency4 = data.groupby([\"CustomerIdx\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency4.columns = [\"CustomerIdx\", \"Week\", \"Recency4\"]\n",
    "    data = data.merge(Recency4, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "    \n",
    "    Recency5 = data.groupby([\"IsinIdx\", \"BuySell\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency5.columns = [\"IsinIdx\", \"BuySell\", \"Week\", \"Recency5\"]\n",
    "    data = data.merge(Recency5, how = \"left\", on = [\"IsinIdx\", \"BuySell\", \"Week\"])\n",
    "    \n",
    "    Recency6 = data.groupby([\"IsinIdx\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency6.columns = [\"IsinIdx\", \"Week\", \"Recency6\"]\n",
    "    data = data.merge(Recency6, how = \"left\", on = [\"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    data.Recency1.fillna(data.Recency1.max(), inplace = True)\n",
    "    data.Recency1.fillna(data.Recency2.max(), inplace = True)\n",
    "    data.Recency1.fillna(data.Recency3.max(), inplace = True)\n",
    "    data.Recency1.fillna(data.Recency4.max(), inplace = True)\n",
    "    data.Recency1.fillna(data.Recency5.max(), inplace = True)\n",
    "    data.Recency1.fillna(data.Recency6.max(), inplace = True)\n",
    "\n",
    "    print(\"Computed 6 recency features...\")\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FREQUENCY (TOTAL NUMBER OF TRADES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FUNCTION TO COMPUTE 6 FREQUENCY-BASED FEATURES \n",
    "# 1) Number of previous customer trades with that bond with the same BuySell direction\n",
    "# 2) Number of previous customer trades with that bond with any direction\n",
    "# 3) Number of previous customer trades with any bond with the same BuySell direction\n",
    "# 4) Number of previous customer trades with any bond with any direction\n",
    "# 5) Number of previous trades with that bond by any of the same BuySell direction\n",
    "# 6) Number of previous trades with that bond by any of the customers\n",
    "# all frequencies are divided by the number of weeks during which they were observed\n",
    "\n",
    "def compute_frequency(data):\n",
    "\n",
    "    tmp = data[data.CustomerInterest == 1]\n",
    "\n",
    "    tmp[\"Frequency1\"] = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).CustomerInterest.cumsum()\n",
    "\n",
    "    data = data.merge(tmp, how = \"left\")\n",
    "    data[\"Frequency1\"] = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).Frequency1.fillna(method = \"ffill\")\n",
    "    data[\"Frequency1\"][data[\"CustomerInterest\"] == 1] = data[\"Frequency1\"][data[\"CustomerInterest\"] == 1] - 1\n",
    "    data[\"Frequency1\"].fillna(0, inplace = True)\n",
    "\n",
    "    Frequency2 = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency2.columns = [\"CustomerIdx\", \"IsinIdx\", \"Week\", \"Frequency2\"]\n",
    "    data = data.merge(Frequency2, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    Frequency3 = data.groupby([\"CustomerIdx\", \"BuySell\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency3.columns = [\"CustomerIdx\", \"BuySell\", \"Week\", \"Frequency3\"]\n",
    "    data = data.merge(Frequency3, how = \"left\", on = [\"CustomerIdx\", \"BuySell\", \"Week\"])\n",
    "\n",
    "    Frequency4 = data.groupby([\"CustomerIdx\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency4.columns = [\"CustomerIdx\", \"Week\", \"Frequency4\"]\n",
    "    data = data.merge(Frequency4, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "\n",
    "    Frequency5 = data.groupby([\"IsinIdx\", \"Week\", \"BuySell\"]).Frequency1.sum().reset_index()\n",
    "    Frequency5.columns = [\"IsinIdx\", \"Week\", \"BuySell\", \"Frequency5\"]\n",
    "    data = data.merge(Frequency5, how = \"left\", on = [\"IsinIdx\", \"Week\", \"BuySell\"])\n",
    "    \n",
    "    Frequency6 = data.groupby([\"IsinIdx\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency6.columns = [\"IsinIdx\", \"Week\", \"Frequency6\"]\n",
    "    data = data.merge(Frequency6, how = \"left\", on = [\"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    div = data.Week - data.Week.min()\n",
    "    div[div == 0] = 1\n",
    "    for var in [\"Frequency1\", \"Frequency2\", \"Frequency3\", \"Frequency4\"]:\n",
    "        data[var] = data[var] / div\n",
    "    \n",
    "    print(\"Computed 6 frequency features...\")\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FUNCTION TO COMPUTE MONTH\n",
    "# 1) Month of each trade\n",
    "\n",
    "def compute_month(data):\n",
    "\n",
    "    tmp = pd.read_csv(\"../data/raw/Trade.csv\")\n",
    "    tmp = tmp[[\"TradeDateKey\"]]\n",
    "\n",
    "    ### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "    def week_idx(date, end_date):\n",
    "        return round((end_date - date).dt.days / 7 + 0.4).astype(int)\n",
    "\n",
    "    tmp[\"TradeDateKey\"] = pd.to_datetime(tmp[\"TradeDateKey\"], format = '%Y%m%d')\n",
    "    tmp[\"Week\"] = week_idx(tmp[\"TradeDateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "    tmp[\"Month\"] = tmp[\"TradeDateKey\"].dt.month.astype(\"object\")\n",
    "    del tmp[\"TradeDateKey\"]\n",
    "    tmp = tmp.drop_duplicates()\n",
    "\n",
    "    data = data.merge(tmp, how = \"left\", on = \"Week\")\n",
    "    data[\"Month\"][data.Week == 121] = \"4\"\n",
    "    \n",
    "    print(\"Computed 1 month feature...\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INDICATORS OF CUSTOMER INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust  = pd.read_csv(\"../data/raw/Customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for customers\n",
    "cust_dummies = pd.concat([cust.CustomerIdx, pd.get_dummies(cust.Sector), pd.get_dummies(cust.Region)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND-SPECIFIC FINANCIAL INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond  = pd.read_csv(\"../data/raw/Isin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bond dummies\n",
    "bond_dummies = pd.concat([bond.IsinIdx,\n",
    "                          pd.get_dummies(bond.ActivityGroup), \n",
    "                          pd.get_dummies(bond.CompositeRating)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACRO VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = pd.read_csv(\"../data/raw/MarketData_Macro.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have >100 variables here:\n",
    "- Stock indices (DAX, FTSE100, ...)\n",
    "- Volatility indices (VSTOXX, VIX, )\n",
    "- Currency exchange rates (USD <> EUR/CNY/...)\n",
    "- Inter-bank money lending rate (Money Market) 3-month for each currency \n",
    "- Mid- to long-term swaps (2-10 years). TODO: Unsure of the effect on bond trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats\n",
    "macro.columns.tolist()[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristically fill missing values with the previous value or 2xprevious value. If still missing, fill values with the following or 2xfollowing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = macro.fillna(macro.shift(1)).fillna(macro.shift(2)).fillna(macro.shift(-1)).fillna(macro.shift(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates\n",
    "macro[\"DateKey\"] = pd.to_datetime(macro[\"DateKey\"], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "def week_idx(date, end_date):\n",
    "    return round((end_date - date).dt.days / 7 + 0.4).astype(int)\n",
    "\n",
    "# add week index\n",
    "macro[\"Week\"] = week_idx(macro[\"DateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "macro[\"Week\"] = macro[\"Week\"].max() + 1 - macro[\"Week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the macro values by week. \n",
    "\n",
    "TODO: We could also take the lag first and then aggregate, not sure what makes more sense (JH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = macro.groupby([\"Week\"]).agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the change in the macro variable compared to the previous date, I think, to check if e.g. the currency value went up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing lag for first week with 0\n",
    "macro_diff1 = (macro - macro.shift(1)).fillna(0)\n",
    "#macro_diff1.columns = [x + \"_diff1\" for x in macro_diff1.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I think it makes sense to create a common variable e.g. \"currency trend\" that relates to the specific currency of the bond and/or holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1 = macro_diff1.filter(like = \"FX\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1[\"USD\"] = 1\n",
    "#fx_diff1[\"USD_diff1\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1 = fx_diff1.reset_index().melt(id_vars = \"Week\", var_name = \"Currency\", value_name = \"Currency_trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1.Currency = fx_diff1.Currency.str[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The currency data 'fx' can be merged into the bond data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond.Currency.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1.Currency.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. COMPUTING FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE FEATURES FROM NIKITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute past features\n",
    "data = compute_recency(data)\n",
    "data = compute_frequency(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute months\n",
    "data = compute_month(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE FEATURES FROM JOHANNES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge customer dummies\n",
    "data = data.merge(cust_dummies, on = \"CustomerIdx\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge bond dummies\n",
    "data = data.merge(bond_dummies, on = \"IsinIdx\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge macro variables\n",
    "data = data.merge(bond[[\"IsinIdx\", \"Currency\"]], on = \"IsinIdx\")\n",
    "data = data.merge(fx_diff1, how = 'left', on = [\"Week\",\"Currency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[fx_diff1.columns] = data[fx_diff1.columns].fillna(0) # There are a few weird currencies (or typos?) for which we don't have information, e.g. CNH. These need to be imputed\n",
    "data.drop(\"Currency\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge exchange rate\n",
    "data = data.merge(macro_diff1.reset_index(), on = \"Week\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE FEATURES FROM ALISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat1_AK\")\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", \"MeanPrice\", \"StdPrice\", \"MeanYield\",\n",
    "      \"StdYield\", \"MeanZScore\", \"StdZScore\", \"YieldMarktDelta\",\n",
    "      \"ZScoreMarktDelta\"]], how = \"left\", on = \"IsinIdx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CORRECT AND EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkr recency distribution\n",
    "data.Recency1.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first X weeks\n",
    "data = data[data.Week > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV\n",
    "data.to_csv(\"../data/prepared/data_v2_0_100.csv\", index = False, compression = \"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
