{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "import matplotlib as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "data = pd.read_csv(\"../data/prepared/data_basic_1.csv\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (111778638, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionIdx</th>\n",
       "      <th>CustomerIdx</th>\n",
       "      <th>IsinIdx</th>\n",
       "      <th>BuySell</th>\n",
       "      <th>CustomerInterest</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16471</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16471</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16471</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16471</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16471</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PredictionIdx  CustomerIdx  IsinIdx BuySell  CustomerInterest  Week\n",
       "0           NaN            0    16471     Buy               0.0     1\n",
       "1           NaN            0    16471     Buy               0.0     2\n",
       "2           NaN            0    16471     Buy               0.0     3\n",
       "3           NaN            0    16471     Buy               0.0     4\n",
       "4           NaN            0    16471     Buy               0.0     5"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data\n",
    "print(\"Dimensions:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FUNCTIONS TO CREATE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "def week_idx(date, end_date):\n",
    "    return round((end_date - date).dt.days / 7 + 0.4).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECENCY (TIME SINCE LAST TRADE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FUNCTION TO COMPUTE 6 RECENCY-BASED FEATURES \n",
    "# 1) Time since last customer trade with that bond with the same BuySell direction\n",
    "# 2) Time since last customer trade with that bond with any direction\n",
    "# 3) Time since last customer trade with any bond with the same BuySell direction\n",
    "# 4) Time since last customer trade with any bond with any direction\n",
    "# 5) Time since last trade with that bond by any of the customers with the same BuySell direction\n",
    "# 6) Time since last trade with that bond by any of the customers\n",
    "\n",
    "def compute_recency(data):\n",
    "\n",
    "    tmp = data[data.CustomerInterest == 1]\n",
    "    \n",
    "    tmp[\"PrevWeek\"] = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).Week.shift(1)\n",
    "    tmp[\"CurrWeek\"] = tmp.Week\n",
    "\n",
    "    data = data.merge(tmp, how = \"left\")\n",
    "\n",
    "    data[\"CurrWeek\"] = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).CurrWeek.fillna(method = \"ffill\")\n",
    "    data[\"PrevWeek\"] = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).PrevWeek.fillna(method = \"bfill\")\n",
    "    data[\"PrevWeek\"][data.PrevWeek.isnull()] = data[\"CurrWeek\"]\n",
    "    data[\"PrevWeek\"][data.PrevWeek >= data.Week] = None\n",
    "    \n",
    "    data[\"Recency1\"] = data[\"Week\"] - data[\"PrevWeek\"]\n",
    "    data[\"Recency1\"][data.Recency1 <= 0] = None\n",
    "    del data[\"PrevWeek\"], data[\"CurrWeek\"]\n",
    "\n",
    "    Recency2 = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency2.columns = [\"CustomerIdx\", \"IsinIdx\", \"Week\", \"Recency2\"]\n",
    "    data = data.merge(Recency2, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    Recency3 = data.groupby([\"CustomerIdx\", \"BuySell\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency3.columns = [\"CustomerIdx\", \"BuySell\", \"Week\", \"Recency3\"]\n",
    "    data = data.merge(Recency3, how = \"left\", on = [\"CustomerIdx\", \"BuySell\", \"Week\"])\n",
    "    \n",
    "    Recency4 = data.groupby([\"CustomerIdx\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency4.columns = [\"CustomerIdx\", \"Week\", \"Recency4\"]\n",
    "    data = data.merge(Recency4, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "    \n",
    "    Recency5 = data.groupby([\"IsinIdx\", \"BuySell\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency5.columns = [\"IsinIdx\", \"BuySell\", \"Week\", \"Recency5\"]\n",
    "    data = data.merge(Recency5, how = \"left\", on = [\"IsinIdx\", \"BuySell\", \"Week\"])\n",
    "    \n",
    "    Recency6 = data.groupby([\"IsinIdx\", \"Week\"]).Recency1.min().reset_index()\n",
    "    Recency6.columns = [\"IsinIdx\", \"Week\", \"Recency6\"]\n",
    "    data = data.merge(Recency6, how = \"left\", on = [\"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    data.Recency1.fillna(data.Recency1.max(), inplace = True)\n",
    "    data.Recency2.fillna(data.Recency2.max(), inplace = True)\n",
    "    data.Recency3.fillna(data.Recency3.max(), inplace = True)\n",
    "    data.Recency4.fillna(data.Recency4.max(), inplace = True)\n",
    "    data.Recency5.fillna(data.Recency5.max(), inplace = True)\n",
    "    data.Recency6.fillna(data.Recency6.max(), inplace = True)\n",
    "    \n",
    "    print(\"Computed 6 recency features...\")\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FREQUENCY (TOTAL NUMBER OF TRADES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FUNCTION TO COMPUTE 6 FREQUENCY-BASED FEATURES \n",
    "# 1) Number of previous customer trades with that bond with the same BuySell direction\n",
    "# 2) Number of previous customer trades with that bond with any direction\n",
    "# 3) Number of previous customer trades with any bond with the same BuySell direction\n",
    "# 4) Number of previous customer trades with any bond with any direction\n",
    "# 5) Number of previous trades with that bond by any of the same BuySell direction\n",
    "# 6) Number of previous trades with that bond by any of the customers\n",
    "# all frequencies are divided by the number of weeks during which they were observed\n",
    "\n",
    "def compute_frequency(data):\n",
    "\n",
    "    tmp = data[data.CustomerInterest == 1]\n",
    "\n",
    "    tmp[\"Frequency1\"] = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).CustomerInterest.cumsum()\n",
    "\n",
    "    data = data.merge(tmp, how = \"left\")\n",
    "    data[\"Frequency1\"] = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"BuySell\"]).Frequency1.fillna(method = \"ffill\")\n",
    "    data[\"Frequency1\"][data[\"CustomerInterest\"] == 1] = data[\"Frequency1\"][data[\"CustomerInterest\"] == 1] - 1\n",
    "    data[\"Frequency1\"].fillna(0, inplace = True)\n",
    "\n",
    "    Frequency2 = data.groupby([\"CustomerIdx\", \"IsinIdx\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency2.columns = [\"CustomerIdx\", \"IsinIdx\", \"Week\", \"Frequency2\"]\n",
    "    data = data.merge(Frequency2, how = \"left\", on = [\"CustomerIdx\", \"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    Frequency3 = data.groupby([\"CustomerIdx\", \"BuySell\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency3.columns = [\"CustomerIdx\", \"BuySell\", \"Week\", \"Frequency3\"]\n",
    "    data = data.merge(Frequency3, how = \"left\", on = [\"CustomerIdx\", \"BuySell\", \"Week\"])\n",
    "\n",
    "    Frequency4 = data.groupby([\"CustomerIdx\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency4.columns = [\"CustomerIdx\", \"Week\", \"Frequency4\"]\n",
    "    data = data.merge(Frequency4, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "\n",
    "    Frequency5 = data.groupby([\"IsinIdx\", \"Week\", \"BuySell\"]).Frequency1.sum().reset_index()\n",
    "    Frequency5.columns = [\"IsinIdx\", \"Week\", \"BuySell\", \"Frequency5\"]\n",
    "    data = data.merge(Frequency5, how = \"left\", on = [\"IsinIdx\", \"Week\", \"BuySell\"])\n",
    "    \n",
    "    Frequency6 = data.groupby([\"IsinIdx\", \"Week\"]).Frequency1.sum().reset_index()\n",
    "    Frequency6.columns = [\"IsinIdx\", \"Week\", \"Frequency6\"]\n",
    "    data = data.merge(Frequency6, how = \"left\", on = [\"IsinIdx\", \"Week\"])\n",
    "    \n",
    "    div = data.Week - data.Week.min()\n",
    "    div[div == 0] = 1\n",
    "    for var in [\"Frequency1\", \"Frequency2\", \"Frequency3\", \"Frequency4\", \"Frequency5\", \"Frequency6\"]:\n",
    "        data[var] = data[var] / div\n",
    "    \n",
    "    print(\"Computed 6 frequency features...\")\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FUNCTION TO COMPUTE MONTH\n",
    "# 1) Month of each trade\n",
    "\n",
    "def compute_month(data):\n",
    "\n",
    "    tmp = pd.read_csv(\"../data/raw/Trade.csv\")\n",
    "    tmp = tmp[[\"TradeDateKey\"]]\n",
    "\n",
    "    ### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "    def week_idx(date, end_date):\n",
    "        return round((end_date - date).dt.days / 7 + 0.4).astype(int)\n",
    "\n",
    "    tmp[\"TradeDateKey\"] = pd.to_datetime(tmp[\"TradeDateKey\"], format = '%Y%m%d')\n",
    "    tmp[\"Week\"] = week_idx(tmp[\"TradeDateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "    tmp[\"Month\"] = tmp[\"TradeDateKey\"].dt.month.astype(\"object\")\n",
    "    del tmp[\"TradeDateKey\"]\n",
    "    tmp = tmp.drop_duplicates()\n",
    "\n",
    "    data = data.merge(tmp, how = \"left\", on = \"Week\")\n",
    "    data[\"Month\"][data.Week == 121] = \"4\"\n",
    "    \n",
    "    print(\"Computed 1 month feature...\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INDICATORS OF CUSTOMER INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust  = pd.read_csv(\"../data/raw/Customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust.loc[cust.Subsector==\"Bank\",\"Sector\"] = \"Bank\"\n",
    "cust.loc[cust.Subsector==\"Broker Dealer\",\"Sector\"] = \"Broker\"\n",
    "cust.loc[cust.Subsector==\"Hedge Fund\",\"Sector\"] = \"Hedgefund\"\n",
    "cust.loc[cust.Subsector==\"Independent Asset Manager\",\"Sector\"] = \"Independent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for customers\n",
    "cust_dummies = pd.concat([cust.CustomerIdx, pd.get_dummies(cust.Sector), pd.get_dummies(cust.Region)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND-SPECIFIC FINANCIAL INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond  = pd.read_csv(\"../data/raw/Isin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['STEP CPN', 'FIXED', 'FLOATING', 'ZERO COUPON', 'VARIABLE', 'NONE'], dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond.CouponType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bond dummies\n",
    "bond_dummies = pd.concat([bond.IsinIdx,\n",
    "                          pd.get_dummies(bond.ActivityGroup), \n",
    "                          pd.get_dummies(bond.CompositeRating),\n",
    "                          pd.get_dummies(bond.CouponType)\n",
    "                         ], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACRO VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = pd.read_csv(\"../data/raw/MarketData_Macro.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have >100 variables here:\n",
    "- Stock indices (DAX, FTSE100, ...)\n",
    "- Volatility indices (VSTOXX, VIX, )\n",
    "- Currency exchange rates (USD <> EUR/CNY/...)\n",
    "- Inter-bank money lending rate (Money Market) 3-month for each currency \n",
    "- Mid- to long-term swaps (2-10 years). TODO: Unsure of the effect on bond trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DateKey',\n",
       " 'SSE',\n",
       " 'DAX',\n",
       " 'EUROSTOXX',\n",
       " 'VSTOXX',\n",
       " 'FTSE100',\n",
       " 'HSI',\n",
       " 'NIKKEI',\n",
       " 'DOWJONES_INDU',\n",
       " 'SP500',\n",
       " 'VIX',\n",
       " 'FX_USD.ARS',\n",
       " 'FX_USD.AUD',\n",
       " 'FX_USD.BRL',\n",
       " 'FX_USD.CAD']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive stats\n",
    "macro.columns.tolist()[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristically fill missing values with the previous value or 2xprevious value. If still missing, fill values with the following or 2xfollowing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = macro.fillna(macro.shift(1)).fillna(macro.shift(2)).fillna(macro.shift(-1)).fillna(macro.shift(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates\n",
    "macro[\"DateKey\"] = pd.to_datetime(macro[\"DateKey\"], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "def week_idx(date, end_date):\n",
    "    return round((end_date - date).dt.days / 7 + 0.4).astype(int)\n",
    "\n",
    "# add week index\n",
    "macro[\"Week\"] = week_idx(macro[\"DateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "macro[\"Week\"] = macro[\"Week\"].max() + 1 - macro[\"Week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the macro values by week. \n",
    "\n",
    "TODO: We could also take the lag first and then aggregate, not sure what makes more sense (JH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = macro.groupby([\"Week\"]).agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the change in the macro variable compared to the previous date, I think, to check if e.g. the currency value went up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing lag for first week with 0\n",
    "macro_diff1 = ((macro - macro.shift(1))/macro).fillna(0)\n",
    "#macro_diff1.columns = [x + \"_diff1\" for x in macro_diff1.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_trend = macro_diff1\\\n",
    "    .filter([\"SSE\",\"DAX\",\"EUROSTOXX\",\"VSTOXX\",\"FTSE100\",\"HSI\",\"NIKKEI\",\"DOWJONAES_INDU\",\"SP500\",\"VIX\"])\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I think it makes sense to create a common variable e.g. \"currency trend\" that relates to the specific currency of the bond and/or holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1 = macro_diff1.filter(like = \"FX\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FX_USD.ARS', 'FX_USD.AUD', 'FX_USD.BRL', 'FX_USD.CAD', 'FX_USD.CHF',\n",
       "       'FX_USD.CNO', 'FX_USD.CNY', 'FX_USD.EUR', 'FX_USD.GBP', 'FX_USD.HKD',\n",
       "       'FX_USD.IDR', 'FX_USD.JPY', 'FX_USD.NOK', 'FX_USD.SGD', 'FX_USD.TRY',\n",
       "       'FX_USD.ZAR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_diff1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1[\"USD\"] = 1\n",
    "#fx_diff1[\"USD_diff1\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1 = fx_diff1.reset_index().melt(id_vars = \"Week\", var_name = \"Currency\", value_name = \"Currency_trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_diff1.Currency = fx_diff1.Currency.str[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOND DATE-RELATED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "bond  = pd.read_csv(\"../data/raw/Isin.csv\")\n",
    "bond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERT DATES TO WEEKS RELATIVE TO THE SAME BASE\n",
    "\n",
    "def week_idx(date, end_date):\n",
    "    return round((end_date - date).dt.days / 7 + 0.4).astype(int)\n",
    "\n",
    "# convert dates\n",
    "bond[\"ActualMaturityDateKey\"] = pd.to_datetime(bond[\"ActualMaturityDateKey\"], format = '%Y%m%d')\n",
    "bond[\"IssueDateKey\"]          = pd.to_datetime(bond[\"IssueDateKey\"], format = '%Y%m%d')\n",
    "\n",
    "# add week index\n",
    "bond[\"ActualMaturityDateKey\"] = week_idx(bond[\"ActualMaturityDateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "bond[\"IssueDateKey\"] = week_idx(bond[\"IssueDateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "\n",
    "# subset\n",
    "bond = bond[[\"IsinIdx\", \"ActualMaturityDateKey\", \"IssueDateKey\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARKET DYNAMIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "market  = pd.read_csv(\"../data/raw/Market.csv\")\n",
    "\n",
    "# convert dates\n",
    "market[\"DateKey\"] = pd.to_datetime(market[\"DateKey\"], format = '%Y%m%d')\n",
    "\n",
    "# add week index\n",
    "market[\"Week\"] = week_idx(market[\"DateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "del market[\"DateKey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weekly difference\n",
    "tmp = market.groupby([\"IsinIdx\", \"Week\"]).agg([\"mean\"])\n",
    "tmp.columns = [\"_lastweekdiff_\".join(col).strip() for col in tmp.columns.values]\n",
    "tmp = tmp.groupby(\"IsinIdx\").diff()\n",
    "tmp = tmp.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weekly SD\n",
    "tmp2 = market.groupby([\"IsinIdx\", \"Week\"]).agg([\"std\"])\n",
    "tmp2.columns = [\"_lastweek_\".join(col).strip() for col in tmp2.columns.values]\n",
    "tmp2 = tmp2.reset_index()\n",
    "tmp2.Week = tmp2.Week + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "market = tmp.merge(tmp2, how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. COMPUTING FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE FEATURES FROM NIKITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute past features\n",
    "data = compute_recency(data)\n",
    "data = compute_frequency(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute months\n",
    "data = compute_month(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute BuySell dummy\n",
    "data[\"Buy\"] = 0\n",
    "data[\"Buy\"][data.BuySell == \"Buy\"] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOND AND MARKET FEATURES\n",
    "\n",
    "# merge data\n",
    "data = data.merge(bond, on = \"IsinIdx\", how = \"left\")\n",
    "data = data.merge(market, on = [\"IsinIdx\", \"Week\"], how = \"left\")\n",
    "\n",
    "# compute day differences\n",
    "data[\"ActualMaturityDateKey\"] = data[\"ActualMaturityDateKey\"] - data[\"Week\"]\n",
    "data[\"IssueDateKey\"]          = data[\"IssueDateKey\"] - data[\"Week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE FEATURES FROM JOHANNES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Purchase distribution moments\n",
    "data = data.merge(\n",
    "    data.groupby([\"CustomerIdx\",\"Week\"]).CustomerInterest.sum().groupby(\"CustomerIdx\").std().reset_index()\\\n",
    "        .filter([\"CustomerIdx\",\"CustomerInterest\"], axis=1)\\\n",
    "        .rename(columns={'CustomerInterest':'week_trading_std'}),\n",
    "    how = \"left\", on = [\"CustomerIdx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge customer dummies\n",
    "data = data.merge(cust_dummies, on = \"CustomerIdx\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge bond dummies\n",
    "bond_dummies = bond_dummies[['IsinIdx', 'FLOW G10', 'FLOW LOCAL MARKET', 'SAS & COVERED BONDS', 'NR']]\n",
    "data = data.merge(bond_dummies, on = \"IsinIdx\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge macro variables\n",
    "data = data.merge(bond[[\"IsinIdx\", \"Currency\"]], on = \"IsinIdx\")\n",
    "data = data.merge(fx_diff1, how = 'left', on = [\"Week\",\"Currency\"])\n",
    "data = data.merge(indices_trend, how = 'left', on = [\"Week\"])\n",
    "\n",
    "# there are a few weird currencies (or typos?) for which we don't have information, e.g. CNH. These need to be imputed\n",
    "data[fx_diff1.columns] = data[fx_diff1.columns].fillna(0)\n",
    "data.drop(\"Currency\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge exchange rates\n",
    "data = data.merge(macro_diff1.reset_index(), on = \"Week\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE FEATURES FROM ALISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD NEW BOND FEATURES [V1]\n",
    "\n",
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat1_AK\")\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", \"MeanPrice\", \"StdPrice\", \"MeanYield\",\n",
    "      \"StdYield\", \"MeanZScore\", \"StdZScore\", \"YieldMarktDelta\",\n",
    "      \"ZScoreMarktDelta\"]], how = \"left\", on = \"IsinIdx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD NEW BOND FEATURES [V2]\n",
    "\n",
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat2_AK\")\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", 'ymeansq', 'ymeancube', 'zmeansq', 'zmeancube',\n",
    "                             'years_tomaturity', 'count', 'ratingspread', 'ratingZspread']], \n",
    "                  how = \"left\", on = \"IsinIdx\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD BOND FEATURES [V3]\n",
    "\n",
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat3_AK\")\n",
    "bond_stat.head()\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", 'predyield', 'predprice']], how = \"left\", on = \"IsinIdx\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CORRECT AND EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del bond_stat, macro_diff1, fx_diff1, bond, bond_dummies, cust_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check recency distribution\n",
    "data.Recency1.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data with too high Recency2\n",
    "data = data.loc[data.Recency2 < 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first X weeks\n",
    "data = data[data.Week > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74680678, 19)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionIdx       26650982\n",
       "CustomerInterest      484758\n",
       "Recency1             1868028\n",
       "Recency3                4114\n",
       "Recency5               16828\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NA\n",
    "nas = data.isnull().sum()\n",
    "nas[nas > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV\n",
    "data.to_csv(\"../data/prepared/data_v8_50_basic.csv\", index = False, compression = \"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
