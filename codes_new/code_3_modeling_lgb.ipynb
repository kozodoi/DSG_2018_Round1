{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import spearmanr\n",
    "import lightgbm as lgb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage collection\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data name (used for saving files too)\n",
    "data_name = \"data_v4_0_60_under\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "data = pd.read_csv(\"../data/prepared/\" + str(data_name) + \".csv\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data name (used for saving files too)\n",
    "data_name = \"data_v4mlp_ak2_0_60_under_rec27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop high recency from train\n",
    "data = data[(data.Week == 121) | (data.Recency2 < 27)]\n",
    "data = data[(data.Week == 121) | (data.Recency1 < data.Recency1.max())]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PREPARATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD MORE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last week sum (CUST)\n",
    "tmp = data.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"Week\"], as_index = True)\n",
    "tmp = tmp.CustomerInterest.sum().reset_index()\n",
    "tmp.columns = [\"CustomerIdx\", \"Week\", \"LastWeekCustSum\"]\n",
    "tmp[\"Week\"] = tmp.Week + 1\n",
    "data = data.merge(tmp, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "print(data.shape)\n",
    "\n",
    "# add last week sum (BOND)\n",
    "tmp = data.sort_values(by = [\"Week\"], ascending = True).groupby([\"IsinIdx\", \"Week\"], as_index = True)\n",
    "tmp = tmp.CustomerInterest.sum().reset_index()\n",
    "tmp.columns = [\"IsinIdx\", \"Week\", \"LastWeekBondSum\"]\n",
    "tmp[\"Week\"] = tmp.Week + 1\n",
    "data = data.merge(tmp, how = \"left\", on = [\"IsinIdx\", \"Week\"])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD PRICE AND NOTIONAL\n",
    "\n",
    "tmp = pd.read_csv(\"../data/prepared/price_notional.csv\", compression = \"gzip\")\n",
    "\n",
    "# add last week price sum (CUST)\n",
    "tmp2 = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"Week\"], as_index = True)\n",
    "tmp2 = tmp2.Price.sum().reset_index()\n",
    "tmp2.columns = [\"CustomerIdx\", \"Week\", \"LastWeekCustPriceSum\"]\n",
    "tmp2[\"Week\"] = tmp2.Week + 1\n",
    "data = data.merge(tmp2, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "print(data.shape)\n",
    "\n",
    "# add last week notional sum (CUST)\n",
    "tmp2 = tmp.sort_values(by = [\"Week\"], ascending = True).groupby([\"CustomerIdx\", \"Week\"], as_index = True)\n",
    "tmp2 = tmp2.NotionalEUR.sum().reset_index()\n",
    "tmp2.columns = [\"CustomerIdx\", \"Week\", \"LastWeekCustNotionalSum\"]\n",
    "tmp2[\"Week\"] = tmp2.Week + 1\n",
    "data = data.merge(tmp2, how = \"left\", on = [\"CustomerIdx\", \"Week\"])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD CUSTOMER INDICATORS\n",
    "\n",
    "cust  = pd.read_csv(\"../data/raw/Customer.csv\")\n",
    "cust.head()\n",
    "\n",
    "# create dummies for customers\n",
    "cust_dummies = pd.concat([cust.CustomerIdx, pd.get_dummies(cust.Subsector)], axis = 1)\n",
    "\n",
    "# merge customer dummies\n",
    "data = data.merge(cust_dummies, on = \"CustomerIdx\", how = \"left\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD BOND FEATURES [V1]\n",
    "\n",
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat1_AK\")\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", \"t\", \"timeOnMarket\", \"percentoflifeleft\"]], how = \"left\", on = \"IsinIdx\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD NEW BOND FEATURES\n",
    "\n",
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat2_AK\")\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", 'ymeansq', 'ymeancube', 'zmeansq', 'zmeancube',\n",
    "                             'years_tomaturity', 'count', 'ratingspread', 'ratingZspread']], \n",
    "                  how = \"left\", on = \"IsinIdx\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD BOND FEATURES [V3]\n",
    "\n",
    "# load bond data\n",
    "bond_stat = pd.read_csv(\"../data/prepared/bondstat3_AK\")\n",
    "bond_stat.head()\n",
    "\n",
    "# merge bond data\n",
    "data = data.merge(bond_stat[[\"IsinIdx\", 'predyield', 'predprice']], \n",
    "                  how = \"left\", on = \"IsinIdx\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOND-SPECIFIC FINANCIAL INDICATORS\n",
    "\n",
    "bond  = pd.read_csv(\"../data/raw/Isin.csv\")\n",
    "\n",
    "# create bond dummies\n",
    "bond_dummies = pd.concat([bond.IsinIdx,\n",
    "                          pd.get_dummies(bond.IndustrySector), \n",
    "                          pd.get_dummies(bond.Region),\n",
    "                          pd.get_dummies(bond.MarketIssue), \n",
    "                          pd.get_dummies(bond.CouponType)], axis = 1)\n",
    "\n",
    "# merge bond dummies\n",
    "data = data.merge(bond_dummies, on = \"IsinIdx\", how = \"left\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKS AND PREPARATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "print(\"Dimensions:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA\n",
    "nas = data.isnull().sum()\n",
    "nas[nas > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features\n",
    "excluded_features = [\"PredictionIdx\", \"CustomerIdx\", \"IsinIdx\", \"BuySell\", \"CustomerInterest\"]\n",
    "features = [var for var in data.columns if var not in excluded_features]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PARTITIONING\n",
    "\n",
    "# training\n",
    "X_train = data.loc[data.Week < 120]\n",
    "y_train = data.loc[data.Week < 120].CustomerInterest\n",
    "\n",
    "# validation\n",
    "X_valid = data.loc[data.Week == 120]\n",
    "y_valid = data.loc[data.Week == 120].CustomerInterest\n",
    "\n",
    "# test set\n",
    "test = data.loc[data.Week == 121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA in test\n",
    "nas = test.isnull().sum()\n",
    "nas[nas > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MODELING - STAGE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "\n",
    "# parallel settings\n",
    "cores = 2\n",
    "\n",
    "# learner settings\n",
    "metric   = \"auc\"\n",
    "verbose  = 250\n",
    "stopping = 100\n",
    "\n",
    "# lightGBM\n",
    "gbm = lgb.LGBMClassifier(n_estimators     = 10000,\n",
    "                         learning_rate    = 0.005,\n",
    "                         num_leaves       = 70,\n",
    "                         colsample_bytree = 0.8,\n",
    "                         subsample        = 0.9,\n",
    "                         max_depth        = 7,\n",
    "                         reg_alpha        = 0.1,\n",
    "                         reg_lambda       = 0.1,\n",
    "                         min_split_gain   = 0.01,\n",
    "                         min_child_weight = 2,\n",
    "                         random_state     = 42,\n",
    "                         num_threads      = cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lightGBM\n",
    "gbm = gbm.fit(X_train[features], y_train, \n",
    "              eval_set = [(X_train[features], y_train), \n",
    "                          (X_valid[features], y_valid)], \n",
    "              eval_metric = metric, verbose = verbose, \n",
    "              early_stopping_rounds = stopping)\n",
    "    \n",
    "# save number of iterations\n",
    "num_iters = gbm.best_iteration_  \n",
    "\n",
    "\n",
    "##### RESULTS (FULL VALIDATION)\n",
    "\n",
    "# k = 61 (v2), train from 80 week, 1k iter, stop = 100 (not met):                0.83679  (0.77887 LB)\n",
    "# k = 61 (v2), train from 80 week, 3k iter, stop = 100 (not met):                0.840938 (0.78581 LB)\n",
    "# k = 61 (v2), train from 80 week, 5k iter, stop = 100 (not met):                0.841801 (0.78626 LB)\n",
    "# k = 61 (v2), train from 80 week, 10k iter, stop = 100 (met):                   0.841866 (0.78626 LB)\n",
    "\n",
    "# k = 62 (v2+v3), train from 80 week, 10k iter, stop = 100 (met):                0.842258 ()\n",
    "# k = 38 (v3),    train from 80 week, 5k iter,  stop = 100 (not met):            0.843073 (0.78504 LB)\n",
    "# k = 39 (v3+BS), train from 80 week, 10k iter, stop = 100 (met):                0.843815 ()\n",
    "# k = 31 (v2red), train from 60 week, 10k iter, stop = 100 (not met):            0.844060 ()\n",
    "\n",
    "# k = 39 (v4), train from 60  week, 10k iter, stop = 100 (met):                  0.844511 ()\n",
    "# k = 39 (v4), train from 80  week, 10k iter, stop = 100 (met):                  0.845086 (0.78952 LB)\n",
    "# k = 39 (v4), train from 100 week, 10k iter, stop = 100 (met):                  0.844058 ()\n",
    "\n",
    "\n",
    "##### RESULTS (UNDERSAMPLED DATA)\n",
    "\n",
    "# k = 39 (v4), under0.5, train from 60  week, 10k iter, stop = 100 (met):        0.843178 (0.79132 LB)\n",
    "# k = 39 (v4), under0.5, train from 80  week, 10k iter, stop = 100 (met):        0.845558 (0.78835 LB)\n",
    "# k = 39 (v4), under0.5, train from 100 week, 10k iter, stop = 100 (met):        0.843116 ()\n",
    "\n",
    "# k = 40 (v4w),        under0.5, train from 60 week, 10k, stop = 100 (met):      0.844871 (0.79152 LB)\n",
    "# k = 44 (v4wlp),      under0.5, train from 60 week, 10k, stop = 100 (met):      0.846500 (0.79330 LB)\n",
    "# k = 46 (v4w2lp),     under0.5, train from 60 week, 10k, stop = 100 (met):      0.846537 ()\n",
    "# k = 52 (v4wlp_ak2),  under0.5, train from 60 week, 10k, stop = 100 (met):      0.846774 (0.79316 LB)\n",
    "# k = 54 (v4w2lp_ak2), under0.5, train from 60 week, 10k, stop = 100 (met):      0.846399 ()\n",
    "\n",
    "\n",
    "##### RESULTS (UNDERSAMPLE, REMOVE RECENCY2 > 27)\n",
    "\n",
    "# 0.786076\n",
    "# 0.786368\n",
    "# 0.787315\n",
    "# 0.789084\n",
    "# 0.789294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### VARIABLE IMPORTANCE\n",
    "\n",
    "# load variable importance\n",
    "importance = pd.DataFrame()\n",
    "importance[\"feature\"] = features\n",
    "importance[\"importance\"] = gbm.feature_importances_\n",
    "\n",
    "# plot variable importance\n",
    "plt.figure(figsize = (10, 20))\n",
    "sns.barplot(x = \"importance\", y = \"feature\", data = importance.sort_values(by = \"importance\", ascending = False))\n",
    "plt.title('LGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot as pdf\n",
    "plt.savefig(\"../var_importance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PREDICTION\n",
    "\n",
    "### VALID DATA\n",
    "\n",
    "# predict validation set\n",
    "pred_valid = X_valid[[\"CustomerIdx\", \"IsinIdx\", \"BuySell\", \"Week\", \"CustomerInterest\"]]\n",
    "pred_valid[\"TARGET\"] = gbm.predict_proba(X_valid[features], num_iteration = num_iters)[:, 1]\n",
    "auc = roc_auc_score(y_valid, pred_valid.TARGET)\n",
    "\n",
    "# export CSV\n",
    "pred_valid.to_csv(\"../pred_valid/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb.csv\", \n",
    "                  index = False, float_format = \"%.8f\")\n",
    "\n",
    "\n",
    "### TEST DATA\n",
    "\n",
    "# predict test set\n",
    "test[\"TARGET\"] = gbm.predict_proba(test[features], num_iteration = num_iters)[:, 1]\n",
    "\n",
    "# check rank correlation with the best submission\n",
    "best = pd.read_csv(\"../submissions/auc789227_ensemble_es.csv.csv\")\n",
    "best = best.merge(test[[\"PredictionIdx\", \"TARGET\"]], how = \"left\", on = \"PredictionIdx\")\n",
    "print(spearmanr(best[\"TARGET\"], best.CustomerInterest))\n",
    "\n",
    "# export CSV\n",
    "subm = best[[\"PredictionIdx\", \"TARGET\"]]\n",
    "subm.columns = [\"PredictionIdx\", \"CustomerInterest\"]\n",
    "subm.to_csv(\"../submissions/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb_1stage.csv\", \n",
    "            index = False, float_format = \"%.8f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV\n",
    "subm = best[[\"PredictionIdx\", \"TARGET\"]]\n",
    "subm.columns = [\"PredictionIdx\", \"CustomerInterest\"]\n",
    "subm.to_csv(\"../submissions/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb_1stage.csv\", \n",
    "            index = False, float_format = \"%.8f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MODELING - STAGE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep top features\n",
    "top = 100\n",
    "features = list(importance[\"feature\"][0:np.min([top, len(features)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full data as train\n",
    "X_train = data.loc[data.Week <= 120]\n",
    "y_train = data.loc[data.Week <= 120].CustomerInterest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrain model with the best iters\n",
    "\n",
    "# lightGBM\n",
    "gbm = lgb.LGBMClassifier(n_estimators     = num_iters,\n",
    "                         learning_rate    = 0.005,\n",
    "                         num_leaves       = 70,\n",
    "                         colsample_bytree = 0.8,\n",
    "                         subsample        = 0.9,\n",
    "                         max_depth        = 7,\n",
    "                         reg_alpha        = 0.1,\n",
    "                         reg_lambda       = 0.1,\n",
    "                         min_split_gain   = 0.01,\n",
    "                         min_child_weight = 2,\n",
    "                         random_state     = 42,\n",
    "                         num_threads      = cores)\n",
    "\n",
    "# train lightGBM\n",
    "gbm = gbm.fit(X_train[features], y_train, \n",
    "              eval_set = [(X_train[features], y_train)], \n",
    "              eval_metric = metric, verbose = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set\n",
    "test[\"TARGET\"] = gbm.predict_proba(test[features], num_iteration = num_iters)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rank correlation with the best submission\n",
    "best = pd.read_csv(\"../submissions/auc789227_ensemble_es.csv.csv\")\n",
    "best = best.merge(test[[\"PredictionIdx\", \"TARGET\"]], how = \"left\", on = \"PredictionIdx\")\n",
    "spearmanr(best[\"TARGET\"], best.CustomerInterest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV\n",
    "subm = best[[\"PredictionIdx\", \"TARGET\"]]\n",
    "subm.columns = [\"PredictionIdx\", \"CustomerInterest\"]\n",
    "subm.to_csv(\"../submissions/auc\" + str(round(auc, 6))[2:8] + \"_\" + str(data_name) + \"_lgb_2stage.csv\", \n",
    "            index = False, float_format = \"%.8f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
