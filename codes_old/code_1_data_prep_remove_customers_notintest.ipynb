{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas options\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION FOR COUNTING MISSINGS\n",
    "def count_missings(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum() / data.isnull().count() * 100).sort_values(ascending = False)\n",
    "    table = pd.concat([total, percent], axis = 1, keys = [\"Total\", \"Percent\"])\n",
    "    table = table[table[\"Total\"] > 0]\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR COMPUTING WEEK INDEX\n",
    "def week_idx(date, end_date):\n",
    "    # TODO: Hows does this work?\n",
    "    return round((end_date - date).dt.days / 7 + 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "test  = pd.read_csv(\"../data/raw/Challenge_20180423.csv\")\n",
    "#cust  = pd.read_csv(\"../data/raw/Customer.csv\")\n",
    "#bond  = pd.read_csv(\"../data/raw/Isin.csv\")\n",
    "#markt = pd.read_csv(\"../data/raw/Market.csv\")\n",
    "#macro = pd.read_csv(\"../data/raw/MarketData_Macro.csv\")\n",
    "trade = pd.read_csv(\"../data/raw/Trade.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check all datasets\n",
    "print(\"Test data:\", test.shape)\n",
    "display(test.head(3))\n",
    "print(\"------------------------------\")\n",
    "print(\"Customer data:\", cust.shape)\n",
    "display(cust.head(3))\n",
    "print(\"------------------------------\")\n",
    "print(\"Bonds data:\", bond.shape)\n",
    "display(bond.head(3))\n",
    "print(\"------------------------------\")\n",
    "print(\"Market data:\", markt.shape)\n",
    "display(markt.head(3))\n",
    "print(\"------------------------------\")\n",
    "print(\"Macroeconomic data:\", macro.shape)\n",
    "display(macro.head(3))\n",
    "print(\"------------------------------\")\n",
    "print(\"Trade data:\", trade.shape)\n",
    "display(trade.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. TRADE & TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOME CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missings\n",
    "#count_missings(trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive stats\n",
    "#trade.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of observations and bonds per customer is different in trade and test data. Trade data only contains bonds that a given customer has actually traded. In test data, for each customer, the set of bonds is only a subset of the bonds that he actually traded in the past (but not the whole set, which leads to a smaller number of observations per customer in the test data). Also, the test set is biased towards very active traders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if bonds in test are a subset of bonds in trade\n",
    "# print \"No\" if bonds in test are new for that customer\n",
    "for i in test.CustomerIdx.unique():\n",
    "    A = trade[trade.CustomerIdx == i].IsinIdx.unique()\n",
    "    B = test[test.CustomerIdx == i].IsinIdx.unique()\n",
    "    C = set(B).issubset(set(A))\n",
    "    if C == False: \n",
    "        print(\"No!\")\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check number of bonds per customer\n",
    "display(trade.groupby(\"CustomerIdx\").IsinIdx.nunique().describe())\n",
    "print(\"------------------------------\")\n",
    "display(test.groupby(\"CustomerIdx\").IsinIdx.nunique().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target equals 0 if TradeStatus = \"Holding\" and 1 in all other cases. The holding operations are artificial and do not actually mean a customer-bond interaction on the considered market, so we drop these observations from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zinovyee.hub\\AppData\\Local\\Continuum\\miniconda3\\envs\\thesis\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32711063748545"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create target variable\n",
    "trade[\"CustomerInterest\"] = 1\n",
    "trade[\"CustomerInterest\"][trade[\"TradeStatus\"] == \"Holding\"] = 0\n",
    "trade.CustomerInterest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting holding cases\n",
    "trade = trade[trade[\"TradeStatus\"] != \"Holding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data does not contain features \"Price\", \"NotionalEUR\", so it is not possible to use them as predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete features\n",
    "del trade[\"NotionalEUR\"]\n",
    "del trade[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates\n",
    "trade[\"TradeDateKey\"] = pd.to_datetime(trade[\"TradeDateKey\"], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add week index\n",
    "trade[\"Week\"] = week_idx(trade[\"TradeDateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "trade[\"Week\"] = trade[\"Week\"].max() + 1 - trade[\"Week\"]\n",
    "test[\"Week\"]  = trade[\"Week\"].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only customers that are also in test\n",
    "trade = trade.loc[trade['CustomerIdx'].isin(set(test.CustomerIdx.unique()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test data, each observation covers one week, whereas the training data is on a daily basis. We can aggregate the training data to a week level to have the same granularity. The target variable is computed as max over a week, whereas for different features we can compute different stats describing behavior during that week (e.g. mean, sd, range, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate weekly data: target = 1 if there is at least single 1 during week\n",
    "trade = trade.groupby([\"CustomerIdx\", \"Week\", \"IsinIdx\", \"BuySell\"], as_index = False).CustomerInterest.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains 0 only if a customer has explicitly stated that she holds a bond. However, there are a lot of 0 missing from the table for the cases when a customer does not interact with a bond at all. Hence, it is necessary to impute these missing observations. For each customer, we can look at the set of bonds that he ever traded and add all missing weeks for each of that bonds as new rows with target = 0 (the rows when customer did not interact with a specific bond). That significantly increases the sample size but also makes data much closer to the test set.\n",
    "\n",
    "Current implementation increases the sample size from 1.7m to 111m rows (~3Gb). This takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_weeks_min = trade.groupby('CustomerIdx',as_index=False)['Week'].min().rename(columns={'Week':'FirstWeekCustomer'})\n",
    "trade_weeks_max = trade.groupby('CustomerIdx',as_index=False)['Week'].max().rename(columns={'Week':'LastWeekCustomer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade[trade.Week>=68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(819549, 5)\n",
      "(26543142, 5)\n"
     ]
    }
   ],
   "source": [
    "# add missing weeks\n",
    "print(trade.shape)\n",
    "trade = trade.groupby([\"CustomerIdx\", \"Week\", \"IsinIdx\", \"BuySell\"]).CustomerInterest.unique().unstack(\"Week\").stack(\"Week\", dropna = False)\n",
    "trade = trade.reset_index()\n",
    "trade.columns = [\"CustomerIdx\", \"IsinIdx\", \"BuySell\", \"Week\", \"CustomerInterest\"]\n",
    "print(trade.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trade.merge(trade_weeks_min, how='left', on='CustomerIdx')\n",
    "trade = trade.merge(trade_weeks_max, how='left', on='CustomerIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72623, 7)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trade = trade.loc[trade.Week>=trade.FirstWeekCustomer,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26543142, 7)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean \"CustomerInterest\" should be around 1.5% according to the organizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03087611104970165"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill new cases with 0 in target\n",
    "trade.CustomerInterest.fillna(0, inplace = True)\n",
    "trade[\"CustomerInterest\"] = trade.CustomerInterest.astype(int)\n",
    "trade.CustomerInterest.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. CUSTOMER DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3471 unique customers in the training data of which 2495 show up in the test set. There are no unknown customers in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unique customers\n",
    "cust.CustomerIdx.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare number of test customers showing up in cust set to number of unique customers in test set\n",
    "np.sum(np.in1d(test.CustomerIdx.unique(), cust.CustomerIdx.unique()))/test.CustomerIdx.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trade.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 20% of customers are interested only once in the training period, more than 50% are interested less than 15 times total -> There are a few customers that are interested a lot, maybe we should focus on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check percentiles\n",
    "np.percentile(trade.loc[trade.CustomerInterest==1].groupby(\"CustomerIdx\").size(), range(0,100,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer information:\n",
    "- 5 different sectors\n",
    "- 41 subsectors\n",
    "- 3 regions\n",
    "- 99 countries\n",
    "\n",
    "We can include sectors and regions directly, but may want to reduce the dimension of subsectors and countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check columns\n",
    "cust.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dummies for customers\n",
    "cust_dummies = pd.concat([cust.CustomerIdx, pd.get_dummies(cust.Sector), pd.get_dummies(cust.Region)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most customers are asset managers or banks/intermediaries (possibly high volume groups?). Distribution over Americas, Europe/Africa, Asia is 53/25/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge customer dummies\n",
    "trade = trade.merge(cust_dummies, on = \"CustomerIdx\", how = \"left\")\n",
    "test  = test.merge(cust_dummies,  on = \"CustomerIdx\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asset Managers/Hedge funds and Banks/Intermediaries show clearly more interest than privat and corporate investors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distribution of customer types\n",
    "trade.loc[trade.CustomerInterest==1,['Asset Managers & Hedge Funds', 'Asset Owners',\n",
    "       'Banks and Intermediaries', 'Corporation', 'Official Institution - OI']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. BONDS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that bonds after maturity are not tradeable. If this is correct, the easiest solution would be to manually correct these to 0 in model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# descriptive stats\n",
    "bond.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bond dummies for model training and merging into trade data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create bond dumiies\n",
    "bond_dummies = pd.concat([bond.IsinIdx,\n",
    "                          bond.Currency, # Used later to merge currency data\n",
    "                          pd.get_dummies(bond.ActivityGroup), \n",
    "                          pd.get_dummies(bond.CompositeRating)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge bond dummies\n",
    "trade = trade.merge(bond_dummies, on = \"IsinIdx\", how = \"left\")\n",
    "test  = test.merge(bond_dummies,  on = \"IsinIdx\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACROECONOMIC DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have >100 variables here:\n",
    "- Stock indices (DAX, FTSE100, ...)\n",
    "- Volatility indices (VSTOXX, VIX, )\n",
    "- Currency exchange rates (USD <> EUR/CNY/...)\n",
    "- Inter-bank money lending rate (Money Market) 3-month for each currency \n",
    "- Mid- to long-term swaps (2-10 years). TODO: Unsure of the effect on bond trades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# descriptive stats\n",
    "macro.columns.tolist()[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristically fill missing values with the previous value or 2xprevious value. If still missing, fill values with the following or 2xfollowing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro = macro.fillna(macro.shift(1)).fillna(macro.shift(2)).fillna(macro.shift(-1)).fillna(macro.shift(-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert dates\n",
    "macro[\"DateKey\"] = pd.to_datetime(macro[\"DateKey\"], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add week index\n",
    "macro[\"Week\"] = week_idx(macro[\"DateKey\"], pd.Timestamp('2018-04-23 00:00:00'))\n",
    "macro[\"Week\"] = macro[\"Week\"].max() + 1 - macro[\"Week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the macro values by week. \n",
    "\n",
    "TODO: We could also take the lag first and then aggregate, not sure what makes more sense (JH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro = macro.groupby([\"Week\"]).agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the change in the macro variable compared to the previous date, I think, to check if e.g. the currency value went up or down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing lag for first week with 0\n",
    "macro_diff1 = (macro - macro.shift(1)).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I think it makes sense to create a common variable e.g. \"currency trend\" that relates to the specific currency of the bond and/or holder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fx_diff1 = macro_diff1.filter(like=\"FX\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fx_diff1[\"USD\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fx_diff1 = fx_diff1.reset_index().melt(id_vars=\"Week\", var_name=\"Currency\", value_name=\"Currency_trend1w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fx_diff1.Currency = fx_diff1.Currency.str[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The currency data 'fx' can be merged into the bond data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fx_diff1.Currency.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trade = trade.merge(fx_diff1, how='left', on=[\"Week\",\"Currency\"])\n",
    "test = test.merge(fx_diff1, how='left', on=[\"Week\",\"Currency\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few weird currencies (or typos?) for which we don't have information, e.g. CNH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trade.Currency_trend1w.fillna(1, inplace=True)\n",
    "test.Currency_trend1w.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. MARKET DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. DATA EXPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check dimensions\n",
    "print(trade.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3470"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade.CustomerIdx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22913"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade.IsinIdx.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerIdx            int64\n",
       "IsinIdx                int64\n",
       "BuySell               object\n",
       "Week                 float64\n",
       "CustomerInterest       int32\n",
       "FirstWeekCustomer    float64\n",
       "LastWeekCustomer     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "types = {'CustomerIdx':'int32', 'IsinIdx':'int32', 'BuySell':'object', 'Week':'int32', 'CustomerInterest':'bool',\n",
    "       'FirstWeekCustomer':'int32', 'Asset Managers & Hedge Funds':'bool', 'Asset Owners':'bool',\n",
    "       'Banks and Intermediaries':'bool', 'Corporation':'bool', 'Official Institution - OI':'bool',\n",
    "       'Americas':'bool', 'Asia Pacific':'bool', 'Europe, Middle East and Africa':'bool',\n",
    "       'Currency':'bool', 'FLOW G10':'bool', 'FLOW LOCAL MARKET':'bool', 'SAS & COVERED BONDS':'bool', 'A':'bool',\n",
    "       'A+':'bool', 'A-':'bool', 'AA':'bool', 'AA+':'bool', 'AA-':'bool', 'AAA':'bool', 'B':'bool', 'B+':'bool',\n",
    "    'B-':'bool', 'BB':'bool', 'BB+':'bool',\n",
    "       'BB-':'bool', 'BBB':'bool', 'BBB+':'bool', 'BBB-':'bool', 'C':'bool', 'C+':'bool', 'CC':'bool', 'CC+':'bool',\n",
    " 'CC-':'bool', 'CCC':'bool',\n",
    "       'CCC+':'bool', 'CCC-':'bool', 'D':'bool', 'DD+':'bool', 'DDD':'bool', 'DDD+':'bool', 'NR':'bool', 'Currency_trend1w':'bool'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trade.A.unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for column in trade.columns:\n",
    "    print(column)\n",
    "    trade[column] = trade[column].astype(types[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(trade_weeks_min, how='left', on='CustomerIdx')\n",
    "test = test.merge(trade_weeks_max, how='left', on='CustomerIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade[\"PredictionIdx\"] = None\n",
    "del test[\"DateKey\"]\n",
    "trade = trade.reindex(test.columns, axis = 1)\n",
    "data = pd.concat([trade, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/prepared/datafull_last_year.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
